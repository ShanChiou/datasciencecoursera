---
title: "Practical Marchine Learning_HK"
author: "Shan-Chiou Lin"
date: "April 10, 2016"
output: html_document
---
###Final Project Report - Practical Machine Learning Course
Homework assignment of Coursera’s MOOC Practical Machine Learning from Johns Hopkins University. For more information about the several MOOCs comprised in this Specialization, please visit: https://www.coursera.org/specialization/jhudatascience/

The scripts have been solely produced, tested and executed on MAC OS X 10.10.5, and RStudio Version   0.98.1103.

For more info: https://github.com/ShanChiou/datasciencecoursera.git

Data Scientis: ShanChiou Lin

###Background Introduction
These are the files produced during a homework assignment of Coursera’s MOOC Practical Machine Learning from Johns Hopkins University. Here is the introduction of the exercise:

“Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement ??? a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).”

###Data Sources
The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project comes from this original source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

Please Note that I the code I use loads the data directly from the URL provided, so that you are not required to download the file to your environment. Please customize the code to your specific needs.

# Getting Data
```{r}
URL_Train <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv";
URL_Test <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv";
Train_Data <- read.csv(url(URL_Train), na.strings=c("NA","#DIV/0!",""));
Test_Data <- read.csv(url(URL_Test), na.strings=c("NA","#DIV/0!",""));
```

## Load Package 
```{r}
library(caret);
library(rattle); 
library(rpart); 
library(rpart.plot);
library(randomForest); 
library(repmis);
library(gbm);
```

## Partioning the training set into two
```{r}
inTrain <- createDataPartition(y=Train_Data$classe, p=0.6, list=FALSE);
Train_Part <- Train_Data[inTrain, ]; 
Valid_Part <- Train_Data[-inTrain, ];
```

## Clean Data 
# Remove NearZeroVariance variables
```{r}
NZV <- nearZeroVar(Train_Part, saveMetrics=TRUE);
NZV_filtered <- nearZeroVar(Train_Part);
Train_Filtered <- Train_Part[-NZV_filtered];
Valid_Filtered <- Valid_Part[-NZV_filtered];
```

#Remove NA
```{r}
mostlyNA <- sapply(Train_Filtered, function(x) mean(is.na(x))) > 0.95;
Train_Filtered_NNA <- Train_Filtered[, mostlyNA==F];
Valid_Filtered_NNA <- Valid_Filtered[, mostlyNA==F];
```

#Remove the first seven predictors 
since these variables have little predicting power for the outcome classe.
```{r}
TrainData <- Train_Filtered_NNA[, -c(1:7)];
ValidData <- Valid_Filtered_NNA[, -c(1:7)];
```

##Model Building
## Rpart Classfication
Consider 5-fold cross validation  Since data transformations may be less important in non-linear models like classification trees, we do not transform any variables.
```{r}
control <- trainControl(method = "cv", number = 5);
fit_rpart <- train(classe ~ ., data = TrainData, method = "rpart", 
                   trControl = control)
print(fit_rpart, digits = 4);
fancyRpartPlot(fit_rpart$finalModel)
```

# predict outcomes using Validation set
```{r}
predict_rpart <- predict(fit_rpart, ValidData)
```

# Show prediction result
```{r}
(conf_rpart <- confusionMatrix(ValidData$classe, predict_rpart))
(accuracy_rpart <- conf_rpart$overall[1])
```
The accuracy rate is around 0.5, and so the out-of-sample error rate is around 0.5. The Model does not predict very well.

## Random forests
```{r}
fit_rf <- randomForest(classe ~ ., data=TrainData)
```

# predict outcomes using validation set
```{r}
predict_rf <- predict(fit_rf, ValidData, type = "class")
```

# Show prediction result
```{r}
conf_rf <- confusionMatrix(predict_rf, ValidData$classe)
(accuracy_rf <- conf_rf$overall[1])
```

```{r}
plot(fit_rf)
```
The Accuracy is around 99%. Random Forest predict very well.

## Generalized Boosted Regression 
```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1);
Fit_gbm <- train(classe ~ ., data=TrainData, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE);
Fit_gbmMod <- Fit_gbm$finalModel;
```

# predict outcomes using validation set
```{r}
predict_gbm <- predict(Fit_gbm, newdata=ValidData)
```

# Show prediction result
```{r}
conf_gbm <- confusionMatrix(predict_gbm, ValidData$classe);
(accuracy_gbm <- conf_gbm$overall[1]);
```
The Accuracy is around 95%. GBM model predict the second best outcome.


Random Forests gave the best Accuracy(99%) in the validation dataset. So Using Random Forest model to predict Test Data

### Predicting Results on the Test Data
```{r}
predict_Test <- predict(fit_rf, Test_Data, type = "class");
predict_Test;
```

# Prepare the submission
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
};
pml_write_files(predict_Test);
```



